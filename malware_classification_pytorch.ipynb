{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "032fdf4f-1c8f-4931-b0ca-95d9ad27b7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightly in c:\\users\\savad\\anaconda3\\lib\\site-packages (1.5.7)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\savad\\anaconda3\\lib\\site-packages (from lightly) (2024.6.2)\n",
      "Requirement already satisfied: hydra-core>=1.0.0 in c:\\users\\savad\\anaconda3\\lib\\site-packages (from lightly) (1.3.2)\n",
      "Requirement already satisfied: lightly-utils~=0.0.0 in c:\\users\\savad\\anaconda3\\lib\\site-packages (from lightly) (0.0.2)\n",
      "Requirement already satisfied: numpy>=1.18.1 in c:\\users\\savad\\anaconda3\\lib\\site-packages (from lightly) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\savad\\anaconda3\\lib\\site-packages (from lightly) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\savad\\anaconda3\\lib\\site-packages (from lightly) (2.31.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\savad\\anaconda3\\lib\\site-packages (from lightly) (1.16.0)\n",
      "Requirement already satisfied: tqdm>=4.44 in c:\\users\\savad\\anaconda3\\lib\\site-packages (from lightly) (4.65.0)"
     ]
    }
   ],
   "source": [
    "!pip install lightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a78334a4-6b77-4df9-8eee-62053fe38f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hscurve_simclr_resnet18) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_score</td><td>▁▂▂▂▃▄▃▃▄▅▆▄▄▆▅▆▆▇▆▆▆▆▆▅▆▇▆▆▆▇▇▇█▆▇▇█▆▇▇</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▇▆▅▅▄▄▅▄▄▃▄▃▃▅▃▃▂▂▂▃▂▃▃▂▂▂▂▂▂▁▂▁▂▁▁▁▃▁▁</td></tr><tr><td>train_accuracy</td><td>▁▁▂▂▃▃▄▅▅▆▆▅▅▆▅▆▆▇▇▇▇▇▇▆▇██▇▇▇▇█▇█▇██▇█▇</td></tr><tr><td>val_accuracy</td><td>▁▂▂▂▃▄▃▃▄▅▅▄▄▆▅▆▆▇▆▆▆▆▆▅▆▇▆▆▆▇▇▇█▆▇▇█▆▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_score</td><td>0.74859</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>loss</td><td>5.34558</td></tr><tr><td>train_accuracy</td><td>0.87891</td></tr><tr><td>val_accuracy</td><td>0.7487</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hscurve_simclr_resnet18</strong> at: <a href='https://wandb.ai/burkinabe/contrastive-learning-pytorch/runs/hscurve_simclr_resnet18' target=\"_blank\">https://wandb.ai/burkinabe/contrastive-learning-pytorch/runs/hscurve_simclr_resnet18</a><br/> View project at: <a href='https://wandb.ai/burkinabe/contrastive-learning-pytorch' target=\"_blank\">https://wandb.ai/burkinabe/contrastive-learning-pytorch</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240624_050154-hscurve_simclr_resnet18\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hscurve_simclr_resnet18). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71872033372141169d3aa84c7b9bdee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011455555555585306, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\savad\\AnacondaProjects\\data-main\\wandb\\run-20240624_104850-mallook_simclr_resnet18</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/burkinabe/contrastive-learning-pytorch/runs/mallook_simclr_resnet18' target=\"_blank\">mallook_simclr_resnet18</a></strong> to <a href='https://wandb.ai/burkinabe/contrastive-learning-pytorch' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/burkinabe/contrastive-learning-pytorch' target=\"_blank\">https://wandb.ai/burkinabe/contrastive-learning-pytorch</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/burkinabe/contrastive-learning-pytorch/runs/mallook_simclr_resnet18' target=\"_blank\">https://wandb.ai/burkinabe/contrastive-learning-pytorch/runs/mallook_simclr_resnet18</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"contrastive-learning-pytorch\",\n",
    "    entity=\"burkinabe\", id=\"mallook_simclr_resnet18\",\n",
    "    config={\n",
    "        \"epochs\": 100\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43b47201-d118-42aa-b4e4-410fd43a23e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "epoch: 00, loss: 6.19323\n",
      "epoch: 00, train accuracy: 0.88216, val accuracy: 0.71875, F1 score: 0.72110\n",
      "epoch: 01, loss: 6.07029\n",
      "epoch: 01, train accuracy: 0.90430, val accuracy: 0.70312, F1 score: 0.70616\n",
      "epoch: 02, loss: 6.02292\n",
      "epoch: 02, train accuracy: 0.91667, val accuracy: 0.73307, F1 score: 0.73409\n",
      "epoch: 03, loss: 5.98915\n",
      "epoch: 03, train accuracy: 0.94661, val accuracy: 0.71354, F1 score: 0.71621\n",
      "epoch: 04, loss: 5.89445\n",
      "epoch: 04, train accuracy: 0.92253, val accuracy: 0.74609, F1 score: 0.74831\n",
      "epoch: 05, loss: 5.94566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\savad\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 05, train accuracy: 0.93294, val accuracy: 0.74479, F1 score: 0.74653\n",
      "epoch: 06, loss: 5.86970\n",
      "epoch: 06, train accuracy: 0.94076, val accuracy: 0.73698, F1 score: 0.73943\n",
      "epoch: 07, loss: 5.86132\n",
      "epoch: 07, train accuracy: 0.93880, val accuracy: 0.74740, F1 score: 0.74943\n",
      "epoch: 08, loss: 5.82639\n",
      "epoch: 08, train accuracy: 0.92578, val accuracy: 0.71484, F1 score: 0.71653\n",
      "epoch: 09, loss: 5.80812\n",
      "epoch: 09, train accuracy: 0.94141, val accuracy: 0.74740, F1 score: 0.74920\n",
      "epoch: 10, loss: 5.77544\n",
      "epoch: 10, train accuracy: 0.95768, val accuracy: 0.76172, F1 score: 0.76391\n",
      "epoch: 11, loss: 5.76799\n",
      "epoch: 11, train accuracy: 0.93359, val accuracy: 0.76172, F1 score: 0.76294\n",
      "epoch: 12, loss: 5.75978\n",
      "epoch: 12, train accuracy: 0.94531, val accuracy: 0.78776, F1 score: 0.78971\n",
      "epoch: 13, loss: 5.72905\n",
      "epoch: 13, train accuracy: 0.94857, val accuracy: 0.77214, F1 score: 0.77413\n",
      "epoch: 14, loss: 5.69941\n",
      "epoch: 14, train accuracy: 0.96354, val accuracy: 0.76823, F1 score: 0.76973\n",
      "epoch: 15, loss: 5.70597\n",
      "epoch: 15, train accuracy: 0.94987, val accuracy: 0.78906, F1 score: 0.78982\n",
      "epoch: 16, loss: 5.68032\n",
      "epoch: 16, train accuracy: 0.96094, val accuracy: 0.78255, F1 score: 0.78366\n",
      "epoch: 17, loss: 5.67084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\savad\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17, train accuracy: 0.95573, val accuracy: 0.77865, F1 score: 0.78008\n",
      "epoch: 18, loss: 5.65097\n",
      "epoch: 18, train accuracy: 0.96745, val accuracy: 0.77214, F1 score: 0.77387\n",
      "epoch: 19, loss: 5.62612\n",
      "epoch: 19, train accuracy: 0.96094, val accuracy: 0.80208, F1 score: 0.80373\n",
      "epoch: 20, loss: 5.62523\n",
      "epoch: 20, train accuracy: 0.96615, val accuracy: 0.77995, F1 score: 0.78181\n",
      "epoch: 21, loss: 5.58319\n",
      "epoch: 21, train accuracy: 0.96354, val accuracy: 0.79948, F1 score: 0.80077\n",
      "epoch: 22, loss: 5.61168\n",
      "epoch: 22, train accuracy: 0.96940, val accuracy: 0.78125, F1 score: 0.78341\n",
      "epoch: 23, loss: 5.64764\n",
      "epoch: 23, train accuracy: 0.96875, val accuracy: 0.80078, F1 score: 0.80224\n",
      "epoch: 24, loss: 5.59957\n",
      "epoch: 24, train accuracy: 0.96745, val accuracy: 0.81120, F1 score: 0.81252\n",
      "epoch: 25, loss: 5.58920\n",
      "epoch: 25, train accuracy: 0.96094, val accuracy: 0.79167, F1 score: 0.79365\n",
      "epoch: 26, loss: 5.56713\n",
      "epoch: 26, train accuracy: 0.95768, val accuracy: 0.79818, F1 score: 0.79904\n",
      "epoch: 27, loss: 5.53248\n",
      "epoch: 27, train accuracy: 0.96159, val accuracy: 0.79427, F1 score: 0.79599\n",
      "epoch: 28, loss: 5.56485\n",
      "epoch: 28, train accuracy: 0.96940, val accuracy: 0.78125, F1 score: 0.78204\n",
      "epoch: 29, loss: 5.55492\n",
      "epoch: 29, train accuracy: 0.97070, val accuracy: 0.81510, F1 score: 0.81648\n",
      "epoch: 30, loss: 5.55951\n",
      "epoch: 30, train accuracy: 0.95964, val accuracy: 0.79167, F1 score: 0.79322\n",
      "epoch: 31, loss: 5.50421\n",
      "epoch: 31, train accuracy: 0.97005, val accuracy: 0.81380, F1 score: 0.81483\n",
      "epoch: 32, loss: 5.51435\n",
      "epoch: 32, train accuracy: 0.97331, val accuracy: 0.81250, F1 score: 0.81317\n",
      "epoch: 33, loss: 5.48113\n",
      "epoch: 33, train accuracy: 0.97396, val accuracy: 0.81771, F1 score: 0.81923\n",
      "epoch: 34, loss: 5.52171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\savad\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34, train accuracy: 0.98307, val accuracy: 0.80729, F1 score: 0.80807\n",
      "epoch: 35, loss: 5.47438\n",
      "epoch: 35, train accuracy: 0.97201, val accuracy: 0.79557, F1 score: 0.79725\n",
      "epoch: 36, loss: 5.49164\n",
      "epoch: 36, train accuracy: 0.97461, val accuracy: 0.81250, F1 score: 0.81390\n",
      "epoch: 37, loss: 5.47930\n",
      "epoch: 37, train accuracy: 0.98568, val accuracy: 0.80990, F1 score: 0.81153\n",
      "epoch: 38, loss: 5.48058\n",
      "epoch: 38, train accuracy: 0.98177, val accuracy: 0.82682, F1 score: 0.82809\n",
      "epoch: 39, loss: 5.46081\n",
      "epoch: 39, train accuracy: 0.96159, val accuracy: 0.81380, F1 score: 0.81556\n",
      "epoch: 40, loss: 5.44166\n",
      "epoch: 40, train accuracy: 0.97721, val accuracy: 0.84375, F1 score: 0.84458\n",
      "epoch: 41, loss: 5.43876\n",
      "epoch: 41, train accuracy: 0.97721, val accuracy: 0.80990, F1 score: 0.81067\n",
      "epoch: 42, loss: 5.46430\n",
      "epoch: 42, train accuracy: 0.97526, val accuracy: 0.79948, F1 score: 0.80055\n",
      "epoch: 43, loss: 5.43992\n",
      "epoch: 43, train accuracy: 0.97721, val accuracy: 0.83203, F1 score: 0.83275\n",
      "epoch: 44, loss: 5.44482\n",
      "epoch: 44, train accuracy: 0.97591, val accuracy: 0.80729, F1 score: 0.80807\n",
      "epoch: 45, loss: 5.38948\n",
      "epoch: 45, train accuracy: 0.98047, val accuracy: 0.80469, F1 score: 0.80602\n",
      "epoch: 46, loss: 5.42146\n",
      "epoch: 46, train accuracy: 0.97396, val accuracy: 0.83073, F1 score: 0.83149\n",
      "epoch: 47, loss: 5.37858\n",
      "epoch: 47, train accuracy: 0.97591, val accuracy: 0.81250, F1 score: 0.81371\n",
      "epoch: 48, loss: 5.37128\n",
      "epoch: 48, train accuracy: 0.97721, val accuracy: 0.82031, F1 score: 0.82147\n",
      "epoch: 49, loss: 5.46041\n",
      "epoch: 49, train accuracy: 0.98698, val accuracy: 0.82943, F1 score: 0.83092\n",
      "epoch: 50, loss: 5.40377\n",
      "epoch: 50, train accuracy: 0.98568, val accuracy: 0.82943, F1 score: 0.82966\n",
      "epoch: 51, loss: 5.45146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\savad\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 51, train accuracy: 0.97917, val accuracy: 0.82552, F1 score: 0.82645\n",
      "epoch: 52, loss: 5.41197\n",
      "epoch: 52, train accuracy: 0.97917, val accuracy: 0.80469, F1 score: 0.80608\n",
      "epoch: 53, loss: 5.36258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\savad\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 53, train accuracy: 0.97526, val accuracy: 0.80729, F1 score: 0.80860\n",
      "epoch: 54, loss: 5.39130\n",
      "epoch: 54, train accuracy: 0.98698, val accuracy: 0.84115, F1 score: 0.84251\n",
      "epoch: 55, loss: 5.38863\n",
      "epoch: 55, train accuracy: 0.98828, val accuracy: 0.83854, F1 score: 0.83927\n",
      "epoch: 56, loss: 5.36048\n",
      "epoch: 56, train accuracy: 0.98958, val accuracy: 0.84766, F1 score: 0.84802\n",
      "epoch: 57, loss: 5.35096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\savad\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 57, train accuracy: 0.97331, val accuracy: 0.84115, F1 score: 0.84199\n",
      "epoch: 58, loss: 5.35899\n",
      "epoch: 58, train accuracy: 0.98177, val accuracy: 0.83594, F1 score: 0.83674\n",
      "epoch: 59, loss: 5.36842\n",
      "epoch: 59, train accuracy: 0.98177, val accuracy: 0.85938, F1 score: 0.85981\n",
      "epoch: 60, loss: 5.34391\n",
      "epoch: 60, train accuracy: 0.98112, val accuracy: 0.82422, F1 score: 0.82556\n",
      "epoch: 61, loss: 5.38370\n",
      "epoch: 61, train accuracy: 0.97786, val accuracy: 0.81510, F1 score: 0.81670\n",
      "epoch: 62, loss: 5.36237\n",
      "epoch: 62, train accuracy: 0.98503, val accuracy: 0.85677, F1 score: 0.85779\n",
      "epoch: 63, loss: 5.35031\n",
      "epoch: 63, train accuracy: 0.98568, val accuracy: 0.85417, F1 score: 0.85494\n",
      "epoch: 64, loss: 5.33675\n",
      "epoch: 64, train accuracy: 0.98698, val accuracy: 0.82161, F1 score: 0.82326\n",
      "epoch: 65, loss: 5.34556\n",
      "epoch: 65, train accuracy: 0.97982, val accuracy: 0.85156, F1 score: 0.85216\n",
      "epoch: 66, loss: 5.36946\n",
      "epoch: 66, train accuracy: 0.98112, val accuracy: 0.83073, F1 score: 0.83176\n",
      "epoch: 67, loss: 5.33244\n",
      "epoch: 67, train accuracy: 0.98568, val accuracy: 0.82812, F1 score: 0.82935\n",
      "epoch: 68, loss: 5.33395\n",
      "epoch: 68, train accuracy: 0.96549, val accuracy: 0.81380, F1 score: 0.81516\n",
      "epoch: 69, loss: 5.32490\n",
      "epoch: 69, train accuracy: 0.98828, val accuracy: 0.85156, F1 score: 0.85229\n",
      "epoch: 70, loss: 5.32775\n",
      "epoch: 70, train accuracy: 0.99089, val accuracy: 0.86849, F1 score: 0.86880\n",
      "epoch: 71, loss: 5.32837\n",
      "epoch: 71, train accuracy: 0.98503, val accuracy: 0.83203, F1 score: 0.83243\n",
      "epoch: 72, loss: 5.33589\n",
      "epoch: 72, train accuracy: 0.99089, val accuracy: 0.85156, F1 score: 0.85272\n",
      "epoch: 73, loss: 5.31240\n",
      "epoch: 73, train accuracy: 0.97721, val accuracy: 0.82943, F1 score: 0.83043\n",
      "epoch: 74, loss: 5.31319\n",
      "epoch: 74, train accuracy: 0.98633, val accuracy: 0.82943, F1 score: 0.83073\n",
      "epoch: 75, loss: 5.29555\n",
      "epoch: 75, train accuracy: 0.98503, val accuracy: 0.84115, F1 score: 0.84217\n",
      "epoch: 76, loss: 5.29724\n",
      "epoch: 76, train accuracy: 0.97917, val accuracy: 0.83333, F1 score: 0.83408\n",
      "epoch: 77, loss: 5.33456\n",
      "epoch: 77, train accuracy: 0.98763, val accuracy: 0.84896, F1 score: 0.84988\n",
      "epoch: 78, loss: 5.29282\n",
      "epoch: 78, train accuracy: 0.98893, val accuracy: 0.85417, F1 score: 0.85516\n",
      "epoch: 79, loss: 5.29512\n",
      "epoch: 79, train accuracy: 0.98307, val accuracy: 0.85286, F1 score: 0.85362\n",
      "epoch: 80, loss: 5.25218\n",
      "epoch: 80, train accuracy: 0.99023, val accuracy: 0.85156, F1 score: 0.85210\n",
      "epoch: 81, loss: 5.29063\n",
      "epoch: 81, train accuracy: 0.98763, val accuracy: 0.86589, F1 score: 0.86663\n",
      "epoch: 82, loss: 5.31538\n",
      "epoch: 82, train accuracy: 0.99154, val accuracy: 0.84505, F1 score: 0.84608\n",
      "epoch: 83, loss: 5.31246\n",
      "epoch: 83, train accuracy: 0.98633, val accuracy: 0.86068, F1 score: 0.86178\n",
      "epoch: 84, loss: 5.28348\n",
      "epoch: 84, train accuracy: 0.98633, val accuracy: 0.83854, F1 score: 0.83927\n",
      "epoch: 85, loss: 5.26515\n",
      "epoch: 85, train accuracy: 0.98503, val accuracy: 0.83073, F1 score: 0.83227\n",
      "epoch: 86, loss: 5.26460\n",
      "epoch: 86, train accuracy: 0.98763, val accuracy: 0.86068, F1 score: 0.86121\n",
      "epoch: 87, loss: 5.25578\n",
      "epoch: 87, train accuracy: 0.97461, val accuracy: 0.83333, F1 score: 0.83447\n",
      "epoch: 88, loss: 5.26146\n",
      "epoch: 88, train accuracy: 0.98503, val accuracy: 0.86068, F1 score: 0.86101\n",
      "epoch: 89, loss: 5.29386\n",
      "epoch: 89, train accuracy: 0.98633, val accuracy: 0.84245, F1 score: 0.84319\n",
      "epoch: 90, loss: 5.26175\n",
      "epoch: 90, train accuracy: 0.99154, val accuracy: 0.85547, F1 score: 0.85627\n",
      "epoch: 91, loss: 5.27289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\savad\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 91, train accuracy: 0.98438, val accuracy: 0.84115, F1 score: 0.84259\n",
      "epoch: 92, loss: 5.30059\n",
      "epoch: 92, train accuracy: 0.98242, val accuracy: 0.85026, F1 score: 0.85096\n",
      "epoch: 93, loss: 5.26881\n",
      "epoch: 93, train accuracy: 0.98372, val accuracy: 0.84896, F1 score: 0.84999\n",
      "epoch: 94, loss: 5.28710\n",
      "epoch: 94, train accuracy: 0.99089, val accuracy: 0.86458, F1 score: 0.86519\n",
      "epoch: 95, loss: 5.30120\n",
      "epoch: 95, train accuracy: 0.99089, val accuracy: 0.86068, F1 score: 0.86174\n",
      "epoch: 96, loss: 5.30220\n",
      "epoch: 96, train accuracy: 0.98438, val accuracy: 0.83984, F1 score: 0.84085\n",
      "epoch: 97, loss: 5.26775\n",
      "epoch: 97, train accuracy: 0.98438, val accuracy: 0.84375, F1 score: 0.84366\n",
      "epoch: 98, loss: 5.23840\n",
      "epoch: 98, train accuracy: 0.98763, val accuracy: 0.87109, F1 score: 0.87185\n",
      "epoch: 99, loss: 5.24924\n",
      "epoch: 99, train accuracy: 0.98568, val accuracy: 0.83594, F1 score: 0.83667\n"
     ]
    }
   ],
   "source": [
    "# Note: The model and training settings do not follow the reference settings\n",
    "# from the paper. The settings are chosen such that the example can easily be\n",
    "# run on a small dataset with a single GPU.\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "from lightly.loss import NTXentLoss\n",
    "from lightly.models.modules import SimCLRProjectionHead\n",
    "from lightly.transforms.simclr_transform import SimCLRTransform\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = SimCLRProjectionHead(512, 512, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "\n",
    "resnet = torchvision.models.resnet18()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "model = SimCLR(backbone)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "transform = SimCLRTransform(input_size=32, gaussian_blur=0.0)\n",
    "#dataset = torchvision.datasets.CIFAR10(\n",
    "#    \"datasets/cifar10\", download=True, transform=transform\n",
    "#)\n",
    "# or create a dataset from a folder containing images or videos:\n",
    "# dataset = LightlyDataset(\"path/to/folder\", transform=transform)\n",
    "# Create the datasets\n",
    "train_dataset = ImageFolder(root='dataval/train', transform=transform)\n",
    "val_dataset = ImageFolder(root='dataval/val', transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "criterion = NTXentLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.06)\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting Training\")\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        (x0, x1), _ = batch\n",
    "        x0 = x0.to(device)\n",
    "        x1 = x1.to(device)\n",
    "        z0 = model(x0)\n",
    "        z1 = model(x1)\n",
    "        loss = criterion(z0, z1)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")\n",
    "\n",
    "    # Feature extraction\n",
    "    model.eval()\n",
    "    train_features, train_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for (x0, x1), labels in dataloader:\n",
    "            x0 = x0.to(device)\n",
    "            features = model.backbone(x0).flatten(start_dim=1)\n",
    "            train_features.append(features.cpu())\n",
    "            train_labels.append(labels)\n",
    "\n",
    "    train_features = torch.cat(train_features).numpy()\n",
    "    train_labels = torch.cat(train_labels).numpy()\n",
    "\n",
    "    # Train a classifier on extracted features\n",
    "    classifier = LogisticRegression(max_iter=1000)\n",
    "    classifier.fit(train_features, train_labels)\n",
    "\n",
    "    # Evaluate on training data\n",
    "    train_predictions = classifier.predict(train_features)\n",
    "    train_accuracy = accuracy_score(train_labels, train_predictions)\n",
    "\n",
    "    # Validation\n",
    "    val_features, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for (x0, x1), labels in val_loader:\n",
    "            x0 = x0.to(device)\n",
    "            features = model.backbone(x0).flatten(start_dim=1)\n",
    "            val_features.append(features.cpu())\n",
    "            val_labels.append(labels)\n",
    "\n",
    "    val_features = torch.cat(val_features).numpy()\n",
    "    val_labels = torch.cat(val_labels).numpy()\n",
    "\n",
    "    # Predictions and metrics\n",
    "    val_predictions = classifier.predict(val_features)\n",
    "    val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    f1 = f1_score(val_labels, val_predictions, average='weighted')\n",
    "\n",
    "    print(f\"epoch: {epoch:>02}, train accuracy: {train_accuracy:.5f}, val accuracy: {val_accuracy:.5f}, F1 score: {f1:.5f}\")\n",
    "    wandb.log({\"epoch\":epoch, \"train_accuracy\": train_accuracy,  \"val_accuracy\": val_accuracy, \"F1_score\":f1, \"loss\": loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "002d40af-f4db-4429-8fd5-a6bdc424f38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 1 1]\n",
      "epoch: 99, accuracy: 0.83984, F1 score: 0.84091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "train_features, train_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for (x0, x1), labels in dataloader:\n",
    "        x0 = x0.to(device)\n",
    "        features = model.backbone(x0).flatten(start_dim=1)\n",
    "        train_features.append(features.cpu())\n",
    "        train_labels.append(labels)\n",
    "train_features = torch.cat(train_features).numpy()\n",
    "train_labels = torch.cat(train_labels).numpy()\n",
    "print(train_labels)\n",
    "# Train a classifier on extracted features\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(train_features, train_labels)\n",
    "# Validation\n",
    "val_features, val_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for (x0, x1), labels in val_loader:\n",
    "        x0 = x0.to(device)\n",
    "        features = model.backbone(x0).flatten(start_dim=1)\n",
    "        val_features.append(features.cpu())\n",
    "        val_labels.append(labels)\n",
    "val_features = torch.cat(val_features).numpy()\n",
    "val_labels = torch.cat(val_labels).numpy()\n",
    "# Predictions and metrics\n",
    "val_predictions = classifier.predict(val_features)\n",
    "accuracy = accuracy_score(val_labels, val_predictions)\n",
    "f1 = f1_score(val_labels, val_predictions, average='weighted')\n",
    "print(f\"epoch: {epoch:>02}, accuracy: {accuracy:.5f}, F1 score: {f1:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015e863-098c-4e35-af67-e44de3283315",
   "metadata": {},
   "outputs": [],
   "source": [
    "8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
